{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tutorial will walk you through implementing a classification \n",
    "#neural network on the popular CIFAR-10 dataset.\n",
    "#CIFAR-10 consists of 32x32 color images, belonging to one of 10 \n",
    "#classes (e.g. automobile, cat, etc.)\n",
    "\n",
    "#You actually don't have to implement anything - just run the cells\n",
    "#and read the comments+code as a guide. \n",
    "\n",
    "#import the standard libraries first and the standard ipython magic functions\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, sys, pickle, random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These parameters are hyperparameters that we \n",
    "#have to search through to find a good value. For now,\n",
    "#I've done most of the hard work for you! With these\n",
    "#hyperparameters you should get around ~85-86% accuracy\n",
    "#(not the state-of-the-art, but perfectly respectable!)\n",
    "\n",
    "batch_size = 128\n",
    "l2_reg_parameter = 0.001\n",
    "lr_decay = 0.92\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check first if we have the files downloaded. If they haven't been,\n",
    "#download them! I was lazy and there is no download progress \n",
    "#marker, but the file is around 200mb so it might take a while.\n",
    "\n",
    "#After running this cell, you wil have three data structures:\n",
    "#data_train is a dictionary with the data and labels for the training set\n",
    "#data_test is a dictionary with the data and labels for the test set\n",
    "#meta_data just simply maps the class index (0-9) to the actual name of the\n",
    "#class (e.g. automobile, cat, etc.)\n",
    "\n",
    "cifar_dir = 'cifar-10-batches-py'\n",
    "batchfiles = glob.glob(cifar_dir+'/*_batch*')\n",
    "\n",
    "if len(batchfiles)!=6:\n",
    "    print('downloading cifar dataset. this may take a little bit...')\n",
    "    os.system('wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz')\n",
    "    print('download complete! unzipping...')\n",
    "    os.system('tar -xvf cifar-10-python.tar.gz')\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('cifar dataset detected. proceeding...')\n",
    "    \n",
    "batchfiles = glob.glob(cifar_dir+'/*_batch*')\n",
    "\n",
    "data_train = {}\n",
    "data_train['data'] = []\n",
    "data_train['labels'] = []\n",
    "for file in batchfiles:\n",
    "    if 'test' in file:\n",
    "        data_batch = pickle.load(open(file,'rb'), encoding='bytes')\n",
    "        data_test = {}\n",
    "        data_test['data'] = data_batch[b'data']\n",
    "        data_test['labels'] = data_batch[b'labels']\n",
    "    else:\n",
    "        data_batch = pickle.load(open(file,'rb'), encoding='bytes')\n",
    "        data_train['data'].append(data_batch[b'data'])\n",
    "        data_train['labels'].append(data_batch[b'labels'])\n",
    "data_train['data'] = np.concatenate(data_train['data'],axis=0)\n",
    "data_train['labels'] = np.concatenate(data_train['labels'],axis=0)\n",
    "\n",
    "meta_data = pickle.load(open(cifar_dir+'/batches.meta','rb'), encoding='bytes')\n",
    "meta_data = [item.decode('utf-8') for item in meta_data[b'label_names']]\n",
    "\n",
    "print('dataset loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize 8 random images in the training set, along with their training labels. \n",
    "#Note that we have to reshape the images and do some transposes before they are \n",
    "#the right shape (32, 32, 3)\n",
    "\n",
    "idxes = np.random.choice(50000,8,replace=False)\n",
    "images = data_train['data'][idxes]\n",
    "\n",
    "f, ax = plt.subplots(1,8, sharey = True, figsize=(16,2))\n",
    "for i in range(8):\n",
    "    ax[i].imshow(np.resize(images[i],[3,32,32]).transpose(1,2,0))\n",
    "    ax[i].set_xlabel(meta_data[data_train['labels'][idxes[i]]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now create the dataset, using the inbuild Dataset class in pytorch.\n",
    "#The Dataset class needs 3 methods defined: __init__, __len__, and __getitem__\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        #initialize the dataset with the actual data\n",
    "        #in general, if you don't want to fit the entire\n",
    "        #dataset into memory (often the case),\n",
    "        #it's often recommended to put an array of \n",
    "        #filenames here\n",
    "        \n",
    "        self.data = data['data']\n",
    "        self.labels = data['labels']\n",
    "        \n",
    "    def __len__(self):\n",
    "        #just has to return the number of entries in the dataset. \n",
    "        #pretty straightforward\n",
    "        \n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #getitem allows you to interact with the Dataset object\n",
    "        #like a standard array. It actually loads the real data in.\n",
    "        #For example you can get the third data point by calling my_dataset[3]\n",
    "        \n",
    "        return {'data':np.resize(np.array(self.data[idx],dtype=np.float32),[3,32,32]),\n",
    "                'label':self.labels[idx]}\n",
    "\n",
    "#We have to initialize one dataset and dataloader for the training set, and one for the test set now.\n",
    "#The num_workers option allows PyTorch to use idle CPUs to lazily load datapoints for you, and\n",
    "#is recommended as it should speed up training. \n",
    "\n",
    "#batch_size is one of the most important hyperparameters in deep learning. It determines\n",
    "#how many datapoints are used per iteration of training your model. Here it is set to 128;\n",
    "#there is no grand consensus on how to pick batch_size, and it is still an active area of research!\n",
    "\n",
    "dataset_train = CIFAR10Dataset(data_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers = 1)\n",
    "\n",
    "dataset_test = CIFAR10Dataset(data_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now get to the fun part: defining your network!\n",
    "\n",
    "\n",
    "#Before we begin, remember the fundamental flow of training a model:\n",
    "#(1) Get input.\n",
    "#(2) Send input through model (forward pass)\n",
    "#(3) Calculate loss at end\n",
    "#(4) Calculate gradients of loss with respect to model parameters\n",
    "#(5) Update model parameters based on gradients\n",
    "\n",
    "#We are slowly building the machinery to accomplish each of these steps. (1) was accomplished through the\n",
    "#Dataset and DataLoader classes. We will build the machinery to accomplish the rest in this cell, and \n",
    "#actually see them in action in the next cell. \n",
    "\n",
    "#Like most object-oriented frameworks, PyTorch has the flow where you define all your objects\n",
    "#and operations first, and then actually put them in action with a real input later.\n",
    "#This cell is where most of the object definitions happen. The next cell we actually see them in action.\n",
    "\n",
    "#Now let's define the most important object: the class that will hold your model definition.\n",
    "\n",
    "class MyFirstNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #The first thing to define is your __init__ function.\n",
    "        #All model classes inherit from nn.Module.\n",
    "        #First we initialize the superclass as follows:\n",
    "        \n",
    "        super(MyFirstNeuralNetwork, self).__init__()\n",
    "        \n",
    "        #Next, we actually create all of the weights that are trained.\n",
    "        #Each of the following calls defines a set of weights along with an operation\n",
    "        #they are associated with.\n",
    "        \n",
    "        #Tecnically, we could just create all the weights here and then define their operatoins later.\n",
    "        #But PyTorch gives you so many nice clean inbuilt functions that it's much easier to do it this way.\n",
    "        #You don't have to worry about each of these right now, just know that each of them\n",
    "        #basically corresponds to a layer that takes in some input and puts out some output.\n",
    "        \n",
    "        self.filter1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 7, padding = 3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        #nn.Sequential is a particularly useful function - it tells the model a specific order you\n",
    "        #want to chain up operations. In this case, we apply filter1, then bn1, then relu1, then pool1\n",
    "        self.firstblock = nn.Sequential(self.filter1, self.bn1, self.relu1, self.pool1)\n",
    "        \n",
    "        self.filter2a = nn.Conv2d(in_channels = 32, out_channels = 128, kernel_size = 3, padding = 1)\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.relu2a = nn.ReLU()\n",
    "        self.filter2b = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "        self.relu2b = nn.ReLU()\n",
    "        self.filter2c = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1)\n",
    "        self.bn2c = nn.BatchNorm2d(128)\n",
    "        self.relu2c = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        self.secondblock = nn.Sequential(self.filter2a, nn.Dropout2d(p=0.2), self.bn2a, self.relu2a, self.filter2b, \\\n",
    "                                         nn.Dropout2d(p=0.2), self.bn2b, self.relu2b, self.filter2c, \\\n",
    "                                         nn.Dropout2d(p=0.2), self.bn2c, self.relu2c, self.pool2)\n",
    "        \n",
    "        self.filter3a = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1)\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.relu3a = nn.ReLU()\n",
    "        self.filter3b = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "        self.relu3b = nn.ReLU()\n",
    "        self.filter3c = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1)\n",
    "        self.bn3c = nn.BatchNorm2d(256)\n",
    "        self.relu3c = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        self.thirdblock = nn.Sequential(self.filter3a, nn.Dropout2d(p=0.2), self.bn3a, self.relu3a, self.filter3b, \\\n",
    "                                         nn.Dropout2d(p=0.2), self.bn3b, self.relu3b, self.filter3c, \\\n",
    "                                         nn.Dropout2d(p=0.2), self.bn3c, self.relu3c, self.pool3)\n",
    "        \n",
    "        \n",
    "        self.filter4a = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.relu4a = nn.ReLU()\n",
    "        self.filter4b = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "        self.relu4b = nn.ReLU()\n",
    "        self.filter4c = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1)\n",
    "        self.bn4c = nn.BatchNorm2d(512)\n",
    "        self.relu4c = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        self.fourthblock = nn.Sequential(self.filter4a, nn.Dropout2d(p=0.2), self.bn4a, self.relu4a, self.filter4b, \\\n",
    "                                         nn.Dropout2d(p=0.2), self.bn4b, self.relu4b, self.filter4c, \\\n",
    "                                         nn.Dropout2d(p=0.2), self.bn4c, self.relu4c, self.pool4)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(2048, 1024), nn.Dropout(p=0.5), nn.BatchNorm1d(1024), nn.ReLU())\n",
    "        self.classifier_head = nn.Linear(1024, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        #This method now actually implements the forward pass of our network (going from input to output)\n",
    "        #Note that happily, we've done basically all of the work already in the __init__ method. \n",
    "        #All we really have to do is chain up everything in a nice neat package.\n",
    "        \n",
    "        N,C,H,W = input.shape\n",
    "        out = self.firstblock(input)\n",
    "        out = self.secondblock(out)\n",
    "        out = self.thirdblock(out)\n",
    "        out = self.fourthblock(out)\n",
    "        \n",
    "        #out.view(N,-1) flattens the out tensor.\n",
    "        out = out.view(N,-1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.classifier_head(out)\n",
    "        \n",
    "        #The output of this model is a vector of 10 probability scores. The higher the score in position i, \n",
    "        #the higher confidence the network has that a certain image belongs to class i.\n",
    "        return out\n",
    "    \n",
    "#Now let's actually instantiate a network.\n",
    "        \n",
    "model = MyFirstNeuralNetwork()\n",
    "\n",
    "#And make sure it runs on GPU\n",
    "model = model.cuda()\n",
    "\n",
    "#Let's define which loss function to use. In this case we use Cross Entropy loss.\n",
    "#We will use this to quantitatively define how far our model predictions are \n",
    "#from the ground truth values. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#We also define our optimizer (usually some variant of gradient descent.)\n",
    "#In this case we use Adam - almost always a safe choice!\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate,weight_decay = l2_reg_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we actually train our model! Let's go through each of the five steps\n",
    "\n",
    "#(1) Get input\n",
    "#(2) Send input through model (forward pass)\n",
    "#(3) Calculate loss at end\n",
    "#(4) Calculate gradients of loss with respect to model parameters\n",
    "#(5) Update model parameters based on gradients\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    #initialize the dataloader for the training set\n",
    "    data_train_enum = enumerate(dataloader_train)\n",
    "\n",
    "    for step, data in data_train_enum:\n",
    "        network_input = data['data'] #network_input now has your input for the model\n",
    "        data_labels = data['label'] #here are the labels (ground truth) for your batch\n",
    "\n",
    "        #Now we use our model (defined in the previous cell) to do the forward pass (step 2)\n",
    "        forward_output = model.forward(network_input.cuda())\n",
    "        \n",
    "        #Calculating the loss is as simple as one line (step 3)\n",
    "        loss = criterion(forward_output, data_labels.cuda())\n",
    "        \n",
    "        #Calculating the gradients is, miraculously, also as simple as one line (step 4)\n",
    "        #PyTorch does all the calculus for you automatically.\n",
    "        loss.backward()\n",
    "        \n",
    "        #Now, all our model weights are populated with their respective gradients.\n",
    "        #For example, if WEIGHT is a parameter of our network, we can now call\n",
    "        #WEIGHT.grad and see that the gradient is nonzero.\n",
    "        #Now we just tell our optimizer to update all our weights with their gradients. (step 5)\n",
    "        #This is, once again, a single command:\n",
    "        optimizer.step()\n",
    "        \n",
    "        #In PyTorch, one overhead you always have to deal with is to always zero out your\n",
    "        #gradients after you're done with them. Just call optimizer.zero_grad() after\n",
    "        #each iteration, or else gradients will persist and continue to accumulate.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step%250==0:\n",
    "            #In this case, every 250 training iterations, I want to check the \n",
    "            #model on the test set and print out the results.\n",
    "            #Note that we don't have to do steps 4-5 here, since\n",
    "            #we should not be updating the model based on the test set.\n",
    "            \n",
    "            #Often in deep learning, the model behavior changes between training\n",
    "            #time and test time. I won't go into why right now, but when you want to test your \n",
    "            #model, always call model.eval() to let the model know that you now want\n",
    "            #to test it. When you are done and want to train it again, call model.train().\n",
    "            model.eval()\n",
    "            \n",
    "            data_test_enum = enumerate(dataloader_test)\n",
    "            accuracy = 0.0\n",
    "            total_N = 0.0\n",
    "            test_loss = 0.0\n",
    "            for step, data in data_test_enum:\n",
    "                \n",
    "                network_input = data['data']\n",
    "                data_labels = data['label']\n",
    "                forward_output = model.forward(network_input.cuda())\n",
    "                loss = criterion(forward_output,data_labels.cuda())\n",
    "                \n",
    "                test_loss += loss.detach()\n",
    "                #Note that we called detach() - this is because we aren't interested in\n",
    "                #evaluating gradients on is test_loss variable. So it frees up some memory.\n",
    "                \n",
    "                #Here we calculate the number of examples that are correctly classified\n",
    "                max_value = torch.argmax(forward_output,1)\n",
    "                accuracy += torch.sum((max_value == data_labels.cuda()).type(torch.FloatTensor))\n",
    "                total_N += network_input.shape[0]\n",
    "            \n",
    "            #Now we print the performance! And then we change the model back to train mode.\n",
    "            print('Epoch: %d, Train Loss: %f, Test Loss: %f, Test Accuracy: %f'%(epoch, loss, batch_size*test_loss/total_N, accuracy/total_N))\n",
    "            model.train()\n",
    "    \n",
    "    #Here we also implement learning rate decay every epoch. \n",
    "    #Just another hyperparameter, don't worry too much about it.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've trained a model we're happy with (hopefully!), so let's save it to disk for later use.\n",
    "\n",
    "torch.save(model.state_dict(),'my_cifar_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we need to load the model, we just call\n",
    "\n",
    "model.load_state_dict(torch.load('my_cifar_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a nifty little script that will let you \n",
    "#visualize some of your results. It will print out\n",
    "#5 random images from the *test* set, along with \n",
    "#percentage confidences the network has on their class.\n",
    "\n",
    "def get_cifar_prediction(input_image):\n",
    "    #assume the input_image is 3x32x32\n",
    "    \n",
    "    if input_image.shape[0] != 3:\n",
    "        input_image = input_image.transpose(2,0,1)\n",
    "        \n",
    "    input_image = np.expand_dims(input_image,0)\n",
    "    input_image = torch.tensor(np.array(input_image,dtype=np.float32)).cuda()\n",
    "    forward_output = model.forward(input_image).detach()\n",
    "    scores = torch.softmax(forward_output,dim=1)\n",
    "    prediction = torch.argmax(scores)\n",
    "    \n",
    "    return prediction.cpu().numpy(), scores.cpu().numpy()[0]\n",
    "\n",
    "model.eval()\n",
    "idxes = np.random.choice(10000,5)\n",
    "\n",
    "f, ax = plt.subplots(1,5,figsize=(15,3),sharey=True)\n",
    "for i in range(5):\n",
    "    test_image = dataset_test[idxes[i]]['data']\n",
    "    prediction,scores = get_cifar_prediction(test_image)\n",
    "    max_scores_sorted = np.argsort(scores)\n",
    "    ax[i].imshow(np.array(test_image,dtype=np.uint8).transpose(1,2,0))\n",
    "    xlabel = '\\n'.join(['%s:%.2f%%'%(meta_data[max_scores_sorted[-i]],100*scores[max_scores_sorted[-i]]) for i in range(1,6)])\n",
    "    ax[i].set_xlabel(xlabel, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
